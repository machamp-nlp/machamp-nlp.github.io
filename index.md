## Welcome

<div style="text-align: justify">
    This website accompanies the Massive Choice, Ample Tasks (MaChAmp) project, which introduces a Toolkit for Multi-task Learning in NLP. 
</div>


## Contributors

<table id='contributor-table'>
  <tr>
    <td>
      <img class="headshots" src='images/rob.jpb' alt='Rob van der Goot'>
    </td>
    <td>
      <img class="headshots" src='images/ahmet.jpg' alt='Ahmet Üstün'>
    </td>
    <td>
      <img class="headshots" src='images/alan.jpg' alt='Alan Ramponi'>
    </td>
    <td>
      <img class="headshots" src='images/barbara.jpg' alt='Barbara Plank'>
    </td>
  </tr>
  <tr>
    <td>
      <div class='names'>Rob van der Goot</div>
    </td>
    <td>
      <div class='names'>Ahmet Üstün</div>
    </td>
    <td>
      <div class='names'>Alan Ramponi</div>
    </td>
    <td>
      <div class='names'>Barbara Plank</div>
    </td>
  </tr>
</table>

## Papers

### Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning in NLP

Rob van der Goot, Ahmet Üstün, Alan Ramponi, Barbara Plank

#### ArXiv
<blockquote>
    <div style="text-align: justify">
        Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy use of fine-tuning BERT-like models in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of NLP tasks in a uniform toolkit, from text classification to sequence labeling and dependency parsing.
    </div> [pdf](https://arxiv.org/abs/2005.14672) | [code](https://github.com/machamp-nlp/machamp)
</blockquote>


<small>Website adapted from: <a href="https://continual-vista.github.io/">https://continual-vista.github.io/</a></small>

